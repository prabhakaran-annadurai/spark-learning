{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346f406f-c4fc-4343-86c1-22507173119a",
   "metadata": {},
   "source": [
    "## Streaming-joins\n",
    "\n",
    "1. Consider Event1 from Source1 needs to be joined with Event2 from source2 based on ID columns.\n",
    "2. Spark keeps the data in memory for a future potential match\n",
    "3. We must specify a time window beyond which spark can safely drop the data from its memory, otherwise OOM issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae82163d-91ac-4538-a0d5-5127ac2ce9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/21 12:11:21 WARN Utils: Your hostname, spark-master, resolves to a loopback address: 127.0.1.1; using 10.168.136.115 instead (on interface ens3)\n",
      "25/07/21 12:11:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 12:11:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, IntegerType\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Streaming join example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25846af9-4d25-4b3f-9852-70f133058a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = StructType() \\\n",
    "    .add(\"id\", StringType()) \\\n",
    "    .add(\"value1\", StringType()) \\\n",
    "    .add(\"event_time\", TimestampType())\n",
    "\n",
    "schema2 = StructType() \\\n",
    "    .add(\"id\", StringType()) \\\n",
    "    .add(\"value2\", StringType()) \\\n",
    "    .add(\"event_time\", TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2168862c-2ff3-4734-9485-62e73e42ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:11:26 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Stream 1: listens on port 9999\n",
    "stream1 = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf5aabb-6ae5-45b6-b570-639adae3020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:11:26 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "# Stream 2: listens on port 9998\n",
    "stream2 = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9998) \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e35af86-0462-405a-ae45-51e5dd199aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON and apply schema\n",
    "df1 = stream1.select(from_json(col(\"value\"), schema1).alias(\"data1\")) \\\n",
    "    .selectExpr(\"data1.id\", \"data1.value1\", \"data1.event_time\") \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")\n",
    "\n",
    "df2 = stream2.select(from_json(col(\"value\"), schema2).alias(\"data2\")) \\\n",
    "    .selectExpr(\"data2.id\", \"data2.value2\", \"data2.event_time\") \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe30e816-7b82-4ce2-9c22-13a0f89f8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.alias(\"df1\")\n",
    "df2 = df2.alias(\"df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b98750e-edb7-43d8-82a9-0c4df4a0c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = df1.join(\n",
    "    df2,\n",
    "    expr(\"\"\"\n",
    "        df1.id = df2.id AND\n",
    "        df1.event_time BETWEEN df2.event_time AND df2.event_time + interval 5 minutes\n",
    "    \"\"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05653e59-2ffc-4f16-b5f4-8dd597022c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:11:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-53c16d3b-154d-4195-b72a-cc31528c92a5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/07/21 12:11:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+------+----------+---+------+----------+\n",
      "| id|value1|event_time| id|value2|event_time|\n",
      "+---+------+----------+---+------+----------+\n",
      "+---+------+----------+---+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+------+-------------------+---+----------+-------------------+\n",
      "| id|value1|         event_time| id|    value2|         event_time|\n",
      "+---+------+-------------------+---+----------+-------------------+\n",
      "|124| click|2025-07-15 10:00:00|124|impression|2025-07-15 09:55:00|\n",
      "+---+------+-------------------+---+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+------+----------+---+------+----------+\n",
      "| id|value1|event_time| id|value2|event_time|\n",
      "+---+------+----------+---+------+----------+\n",
      "+---+------+----------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = joined.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49aa9dda-1926-41cc-bda2-201767fc7c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae5c52-2748-4ad7-9981-69d22f02830f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd85cd3a-5672-461a-91b3-402b5e4b20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15114294-ef17-44ec-b5a2-6f0c267157e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
