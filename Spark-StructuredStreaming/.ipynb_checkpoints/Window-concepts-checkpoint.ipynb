{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65350ebf-6a2a-47b9-be49-2787240f2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import StructType, StringType, TimestampType, IntegerType\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76922bfd-86a9-4fb3-8db5-34988e44350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"eventId\", IntegerType()) \\\n",
    "    .add(\"timestamp\", TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e622bc64-b806-46fb-8025-25e1b328bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (1, datetime(2025, 7, 18, 12, 0, 3)),\n",
    "    (2, datetime(2025, 7, 18, 12, 5, 7)),\n",
    "    (3, datetime(2025, 7, 18, 12, 9, 12)),\n",
    "    (4, datetime(2025, 7, 18, 12, 10, 17)),\n",
    "    (5, datetime(2025, 7, 18, 12, 15, 17)),\n",
    "    (6, datetime(2025, 7, 18, 12, 20, 27)),\n",
    "    (7, datetime(2025, 7, 18, 12, 22, 7)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "532806e4-f027-497f-b70c-025a0ebee408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b8264-cb09-4c47-b4db-d5ebbf08220a",
   "metadata": {},
   "source": [
    "### 1. What if I need to count the number of events in 10 minutes interval ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12d7f645-4817-41b2-978a-80f3708a2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL like option - Add a column representing window_start time\n",
    "\n",
    "df2 = df.withColumn('hour',sf.date_trunc('hour',df.timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27d1f54b-b15c-4b2f-b5ea-c3747cad3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn('minutes', sf.minute(df.timestamp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93d6a16a-4116-4bcb-8246-cc02953f0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.withColumn('window_start', sf.timestamp_add('minute',sf.lit(10* sf.floor(df3.minutes/10)) ,df3.hour ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7ad363e2-eb75-4139-861c-12a8f5c97733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------+-------------------+\n",
      "|eventId|          timestamp|               hour|minutes|       window_start|\n",
      "+-------+-------------------+-------------------+-------+-------------------+\n",
      "|      1|2025-07-18 12:00:03|2025-07-18 12:00:00|      0|2025-07-18 12:00:00|\n",
      "|      2|2025-07-18 12:05:07|2025-07-18 12:00:00|      5|2025-07-18 12:00:00|\n",
      "|      3|2025-07-18 12:09:12|2025-07-18 12:00:00|      9|2025-07-18 12:00:00|\n",
      "|      4|2025-07-18 12:10:17|2025-07-18 12:00:00|     10|2025-07-18 12:10:00|\n",
      "|      5|2025-07-18 12:15:17|2025-07-18 12:00:00|     15|2025-07-18 12:10:00|\n",
      "|      6|2025-07-18 12:20:27|2025-07-18 12:00:00|     20|2025-07-18 12:20:00|\n",
      "|      7|2025-07-18 12:22:07|2025-07-18 12:00:00|     22|2025-07-18 12:20:00|\n",
      "+-------+-------------------+-------------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38d884ca-bfad-4b47-bc88-115a5518c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|       window_start|count|\n",
      "+-------------------+-----+\n",
      "|2025-07-18 12:00:00|    3|\n",
      "|2025-07-18 12:10:00|    2|\n",
      "|2025-07-18 12:20:00|    2|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.groupby('window_start').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a9e7de-3350-4b06-81f1-d014f4711387",
   "metadata": {},
   "source": [
    "## Pyspark window options\n",
    "\n",
    "1. Tumbling window - |--10 mins ---|---10 mins---|---\n",
    "2. Sliding window - overlapping Tumbling window |-----10 mins----|\n",
    "                                                       |---- 10 mins----|\n",
    "                                                               |------10 mins ----|\n",
    "\n",
    "3. Session windows - Arbitrary length windows - can be dynamic based on a column value\n",
    "                        |-----15 mins ----|         |----2 mins----|    |-------14mins ----------|\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07b4a649-a601-441c-9992-21b1cba410a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2025-07-18 12:00...|    3|\n",
      "|{2025-07-18 12:10...|    2|\n",
      "|{2025-07-18 12:20...|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pyspark window option (Tumbling window)\n",
    "df2=df.groupby(sf.window(df.timestamp,'10 minutes')).count()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "501197b9-daee-42e6-95f4-6adc4fd24008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2025-07-18 12:05...|    3|\n",
      "|{2025-07-18 12:00...|    3|\n",
      "|{2025-07-18 11:55...|    1|\n",
      "|{2025-07-18 12:10...|    2|\n",
      "|{2025-07-18 12:20...|    2|\n",
      "|{2025-07-18 12:15...|    3|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sliding window with overlapping\n",
    "df.groupby(sf.window(df.timestamp,'10 minutes','5 minutes')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0da38424-4825-4934-9ea5-941b18851b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9f78bcf7-6845-4e3c-a7ab-23714a2005e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/19 01:06:18 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "#df = spark.createDataFrame( [[\"2025-7-18 12:03:10 hello world\"]],\"lines  String\" )\n",
    "df = spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",9001).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1b0a2c4a-8aa1-4923-954c-33effb2f5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.withColumn('SplitData',sf.split(df.value,\" \")).\\\n",
    "                        withColumn('TimeStamp'\n",
    "                        ,sf.concat( sf.col('SplitData')[0].cast(\"string\")                                                            \n",
    "                        ,sf.lit(\" \")\n",
    "                        ,sf.col('SplitData')[1].cast(\"string\")\n",
    "                        )).\\\n",
    "                        withColumn(\"lineData\"\n",
    "                        ,sf.slice (sf.col(\"SplitData\"),3,sf.size(sf.col(\"SplitData\")))\n",
    "                        ).\\\n",
    "                        withColumn(\"word\",sf.explode(sf.col(\"lineData\"))).\\\n",
    "                        withColumn(\"TimeStamp\",sf.to_timestamp(sf.col(\"TimeStamp\"))).\\\n",
    "                        select(\"TimeStamp\",\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1b3fa712-19b6-408d-9df0-319b8932f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df2.groupby(sf.window(df2.TimeStamp,'10 minutes')).count()\n",
    "df3.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5904f3b3-2c75-45de-820b-7a14da7ec33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TimeStamp: timestamp (nullable = true)\n",
      " |-- word: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9bc0fc74-0ce1-4255-abb4-a697a821076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/19 01:14:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6c624bbd-e464-4a43-8032-1bb582552099. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/07/19 01:14:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7905d7b233b0>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|window|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2025-07-18 12:00...|    4|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2025-07-18 12:00...|    4|\n",
      "|{2025-07-18 11:50...|    3|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2025-07-18 12:00...|    4|\n",
      "|{2025-07-18 11:50...|    7|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|              window|count|\n",
      "+--------------------+-----+\n",
      "|{2025-07-18 12:00...|    4|\n",
      "|{2025-07-18 11:00...|    4|\n",
      "|{2025-07-18 11:50...|    7|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=df2.withWatermark(\"TimeStamp\", \"10 minutes\").groupby(sf.window(df2.TimeStamp,'10 minutes')).count()\n",
    "df3.writeStream.outputMode(\"update\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "69566219-fa06-4085-a7b5-15ffc64d5651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/19 01:17:19 WARN TextSocketMicroBatchStream: Stream closed by localhost:9001\n",
      "25/07/19 01:17:44 WARN StateStore: Error running maintenance thread\n",
      "java.lang.IllegalStateException: SparkEnv not active, cannot do maintenance on StateStores\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.doMaintenance(StateStore.scala:950)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$.$anonfun$startMaintenanceIfNeeded$1(StateStore.scala:924)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask$$anon$1.run(StateStore.scala:725)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a312c82-9f7a-4d9e-a287-d0ca61053a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
